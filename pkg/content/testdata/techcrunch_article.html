<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI launches new GPT-4 Turbo model with vision capabilities | TechCrunch</title>
    <meta name="description" content="OpenAI announced GPT-4 Turbo with vision capabilities at its DevDay conference">
    <meta property="og:title" content="OpenAI launches new GPT-4 Turbo model with vision capabilities">
    <meta property="og:description" content="The new model features a 128K context window and knowledge up to April 2023">
</head>
<body>
    <!-- Header -->
    <header>
        <nav>
            <div class="logo">TechCrunch</div>
            <ul>
                <li><a href="/startups">Startups</a></li>
                <li><a href="/venture">Venture</a></li>
                <li><a href="/security">Security</a></li>
                <li><a href="/ai">AI</a></li>
            </ul>
        </nav>
    </header>

    <!-- Social sharing buttons -->
    <div class="share-buttons">
        <button>Share on Twitter</button>
        <button>Share on Facebook</button>
        <button>Share on LinkedIn</button>
    </div>

    <!-- Main content -->
    <main>
        <article>
            <header>
                <h1>OpenAI launches new GPT-4 Turbo model with vision capabilities</h1>
                <div class="article-meta">
                    <span class="author">Kyle Wiggers</span>
                    <time datetime="2023-11-06">November 6, 2023</time>
                    <span class="read-time">5 min read</span>
                </div>
            </header>

            <div class="article-content">
                <p>OpenAI today announced GPT-4 Turbo, a new version of its flagship large language model that the company says is "more capable" and offers a 128K context window â€” equivalent to 300 pages of text in a single prompt.</p>

                <p>The new model, unveiled at OpenAI's first developer conference DevDay in San Francisco, also includes several other improvements including more up-to-date knowledge (with a cutoff date of April 2023), the ability to call multiple functions in a single message, and significantly lower pricing.</p>

                <h2>Vision capabilities and multimodal support</h2>

                <p>Perhaps most notably, GPT-4 Turbo with vision is now available through the API, allowing developers to build applications that can process images alongside text. This brings the vision capabilities that were previously only available in ChatGPT to the broader developer ecosystem.</p>

                <p>"GPT-4 Turbo can accept images as inputs in the API, enabling use cases such as generating captions, analyzing real-world images in detail, and reading documents with figures," OpenAI wrote in a blog post.</p>

                <h2>Pricing and performance</h2>

                <p>The company is also dramatically reducing prices. GPT-4 Turbo is priced at $0.01 per 1,000 input tokens and $0.03 per 1,000 output tokens, making it 3x cheaper for input tokens and 2x cheaper for output tokens compared to the original GPT-4.</p>

                <blockquote>
                    <p>"We're able to offer these lower prices because of efficiencies we've found in our systems," said Sam Altman, OpenAI CEO, during the keynote.</p>
                </blockquote>

                <h2>Function calling and JSON mode</h2>

                <p>The new model introduces improved function calling, allowing multiple functions to be called in a single message. It also features a new JSON mode that ensures the model will respond with valid JSON, making it easier to build applications that require structured data.</p>

                <p>OpenAI also announced that GPT-4 Turbo performs better on tasks that require following instructions carefully, and is less likely to return "lazy" responses where the model doesn't complete a task.</p>

                <h2>Availability and rollout</h2>

                <p>GPT-4 Turbo is available starting today in preview for all paying developers, with stable production-ready models expected in the coming weeks. The company plans to automatically upgrade applications using the stable GPT-4 model to GPT-4 Turbo upon its release.</p>

                <p>Developers can access GPT-4 Turbo by passing "gpt-4-1106-preview" as the model name in the API. The vision-enabled version is available as "gpt-4-vision-preview".</p>
            </div>

            <!-- Author bio -->
            <div class="author-bio">
                <h3>About the author</h3>
                <p>Kyle Wiggers is a senior reporter at TechCrunch covering AI and machine learning.</p>
            </div>
        </article>

        <!-- Sidebar -->
        <aside>
            <div class="related-articles">
                <h3>Related Articles</h3>
                <ul>
                    <li><a href="#">Microsoft announces new AI features for Office</a></li>
                    <li><a href="#">Google updates Bard with image generation</a></li>
                    <li><a href="#">Meta releases Code Llama 2</a></li>
                </ul>
            </div>

            <div class="newsletter">
                <h3>Newsletter</h3>
                <p>Get the latest AI news delivered to your inbox</p>
                <form>
                    <input type="email" placeholder="Enter your email">
                    <button>Subscribe</button>
                </form>
            </div>
        </aside>
    </main>

    <!-- Footer -->
    <footer>
        <p>&copy; 2023 TechCrunch. All rights reserved.</p>
        <nav>
            <a href="/privacy">Privacy Policy</a>
            <a href="/terms">Terms of Service</a>
        </nav>
    </footer>

    <!-- Ads -->
    <div class="ads">
        <div class="ad-banner">Advertisement</div>
    </div>
</body>
</html>